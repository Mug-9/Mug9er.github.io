---
title: 吴恩达机器学习5
mathjax: true
categories:
  - 机器学习
  - 学习笔记
tags:
  - 机器学习
  - 学习笔记
abbrlink: 7e4dc831
date: 2021-07-03 14:32:32
img: https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/cover/wallhaven-k7zv97.2u8spfxicri0.jpg
---

吴恩达机器学习笔记(五)

<!--less-->

# 代价函数

## 神经网络

![命名规则](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/image-20210704195854656.2lsjkjwd5e80.png)

使用$L$来代表神经网络的层数

使用$s_l$来代表第$l$层的神经元数目

## 代价函数

![代价函数](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/image-20210704200652152.6e3sjb675mo0.png)

相对于单个的逻辑回归的代价函数，整个神经网络的代价函数只对输出层进行计算，计算第$i$的输出与训练集的结果$y_i$之间的差距。因为神经网络的数据不是唯一，可能由多个分类的输出，所以$K$代表输出的分类个数。计算正则化也是如何，跟逻辑回归类似，不计算0下表的$\Theta$

# 反向传播算法

## 向前传播方法

向前传播方法是前面用到的方法

![向前传播方法](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/image-20210704201536262.33e5qpshz1o0.png)

从输入开始，对每一层进行计算得到下一层的结果，以此往复得到最终结果

## 反向传播方法

方向传播方法从直观上说就是对每一个节点计算$\delta_j^{(l)}$代表第$l$层，第$j$个节点的误差

![反向传播方法](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/image-20210704205325691.3i6ni2tf3u40.png)

还是使用4层的神经网络来说

$\delta_j^{(4)}=a_j^{(4)}-y_j$

第四层就是输出值和$y$值的误差，然后我们反向传播到第三层

$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)} .* g'(z^{(3)})$

$\delta^{(3)}$由参数$\Theta^{(3)}$转置叉乘$\delta^{(4)}$然后点乘(这里点乘只是各个数字相乘) $g(z^{(3)})$的导数，

>对$g$函数求导，先设
>
>$a = g(z) = \frac{1}{1+e^{-z}} $
>
>$g'(z) \\ = (\frac{1}{1+e^{-z}})' \\ = \frac{1'(\frac{1}{1+e^{-z}})+1(\frac{1}{1+e^{-z}})'}{(1+e^{-z})^2} \\ = \frac{e^{-z}}{(1+e^{-z})^2} \\ = \frac{e^{-z}+1-1}{(1+e^{-z})^2} \\ = \frac{(e^{-z}+1) -1}{(1+e^{-z})^2} \\ = \frac{1}{(1+e^{-z})}-\frac{1}{(1+e^{-z})^2} \\ = a - a^2 \\ = a(1-a)$

那么现在$g'(z^{(3)})$就为$a^{(3)}.*(1-a^{(3)})$

同样的$\delta^{(2)}$也是如此

$\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)} .* g'(z^{(2)})$

在不严谨的情况下，我们可以得到初略的偏导数$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{l+1}$ 这些都是忽略的正则化的情况下

### 反向传播算法

![反向传播算法](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/image-20210704205722145.5eidr8is6h00.png)

一开始我们将所有的$\Delta$设置为0，$\Delta$是$\delta$的大写形式

然后我们每一层计算$a^{l}$,计算到输出层以后倒回来计算$\delta$,一直计算到$\delta^2$

然后计算$\Delta$,$\Delta$的计算公式:$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$

向量式写法可以写成$\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$

最终计算$D$,$  \begin{array}{lr} D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}  & if\ j \not=0 \\ D_{ij}^(l) := \frac{1}{m}\Delta_{ij}^{(l)} & if\ j = 0 \end{array} $

这里加上了正则化

最终的偏导正好等于$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$

### 理解反向传播算法

以单一的输出并且忽略$\lambda$为例，代价函数可以写成$cost(i)=y^{(i)}\log h_{\Theta}(x^{(i)})+(1-y^{(i)})\log h_{\Theta}(x^{(i)})$

![理解反向传播算法](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/image-20210704212854863.47c4lnlvmdy0.png)

最终的代价值是由每个节点的误差累计而成，所以对于最终的带价值，每一个$\delta_j^{(l)}$都相当于这个代价的偏导，由这些误差联合作用得到最终的误差值，而每一个$\delta_{i}^{(l)}$都可以从后面的$\delta$推导出来，比如$\delta_2^{2}=\Theta_{12}^{(2)}\delta_{1}^{(3)}+\Theta_{22}^{(2)}\delta^{(3)}_2$

这样就可以得出$\delta$的推导公式$\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}$，至于那么求导暂时没搞懂

