---
title: 吴恩达机器学习作业
mathjax: true
abbrlink: 74ea918c
date: 2021-06-25 20:05:15
img: https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/cover/wallhaven-l3yko2.11toafb13xow.jpg
categories: [机器学习, 作业]
tags: [机器学习, 作业]
---

吴恩达机器学习作业

<!-- less --> 

# 第一周

## 基础

### Part 1: Basic Function

#### $WarmUpExercise$

```octave
function A = warmUpExercise()
% ============= YOUR CODE HERE ==============
A = eye(5);
% ===========================================
end
```

### Part 2: Plotting

#### $PlotData$

```octave
function plotData(x, y)
% ====================== YOUR CODE HERE ======================
figure(1);
plot(X, y, 'r');
xlabel("x");
ylabel("y");
% ===========================================================
end
```

### Part 3: Cost and Gradient descent

#### $computeCost$

且代价函数直接套公式

$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$

```octave
function J = computeCost(X, y, theta)
m = length(y); % number of training examples
% You need to return the following variables correctly 
J = 0;
% ====================== YOUR CODE HERE ======================
J = X * theta - y;
J = 1/(2*m) * sum(J .^ 2);
% =========================================================================
end
```

#### $gradientDescent$

直接套公式即可

$Repeat \{  \\ \theta_j:=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j \\ \}$

```octave
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);
for iter = 1:num_iters
    % ====================== YOUR CODE HERE ======================
    theta = theta - alpha/m * X' * (X*theta - y);
    % ============================================================
    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
end
end
```

## 附加

### Part 1: Feature Normalization

#### $featureNormalize$

特征放缩，每个特征减去平均值并除以方差

```octave
function [X_norm, mu, sigma] = featureNormalize(X)
X_norm = X;
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));
% ====================== YOUR CODE HERE ======================    
mu = mean(X);
sigma = std(X);
for i = 1:size(X, 1)
  for j = 1:size(mu, 2)
    X_norm(i,j) = (X_norm(i,j) - mu(1,j)) / sigma(1, j);
  endfor
end
% ============================================================
end
```

### Part 2: Gradient Descent

#### $computeCostMulti$

套公式

```octave
function J = computeCostMulti(X, y, theta)
m = length(y); % number of training examples
J = 0;
% ====================== YOUR CODE HERE ======================
%               You should set J to the cost.
J = 1 / (2 * m) * sum(((X * theta) - y) .^ 2);
% =========================================================================
end
```

#### $gradientDescentMulti$

跟单个特征的算法类似，直接套公式即可

```octave
function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);
for iter = 1:num_iters
    % ====================== YOUR CODE HERE ======================
  theta = theta - alpha / m * (X' * (X * theta - y));
    % ============================================================
    % Save the cost J in every iteration    
    J_history(iter) = computeCostMulti(X, y, theta); 
end
end
```

### Part 3: Normal Equations

#### $normal Equations$

直接套公式即可

$\theta=(X^TX)^{-1}X^Ty$

```octave
function [theta] = normalEqn(X, y)
theta = zeros(size(X, 2), 1);
% ====================== YOUR CODE HERE ======================
theta = pinv((X'*X))*X'*y
% ============================================================
end
```

