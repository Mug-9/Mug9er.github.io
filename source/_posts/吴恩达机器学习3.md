---
title: 吴恩达机器学习3
mathjax: true
img: >-
  https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/cover/wallhaven-dp3yg3.qt5rkp8gx68.jpg
categories:
  - 机器学习
  - 学习笔记
tags:
  - 机器学习
  - 学习笔记
abbrlink: 972e6d04
date: 2021-06-25 21:37:42
---

吴恩达机器学习笔记(三)

<!--less-->

## 逻辑回归

### 二元分类

二元分类中$y$值只能取两个值，0或1

$y=\{ 0, 1\}$

一般的话，0代表没有，1代表有

### 逻辑回归

逻辑回归实际是一种分类算法，逻辑回归的输出值在0到1之间

$0\le h_{\theta}(x)\le1$

线性回归的一般公式是：$h_{\theta}(x)=\theta^Tx$,得到的是一个数值

而逻辑回归的公式则为：$h_{\theta}(x)=g(\theta^Tx)\\g(z)=\frac{1}{1+e^{-z}}$

$g$函数一般被称为 $sigmoid\ $函数或 $logistic\ $函数

![逻辑函数](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/logisic.3eotdbigmhu0.png)

逻辑函数$h$的曲线在0到1之间，也可以说逻辑曲线的值参数$x$为1的可能性

$h_{\theta}(x)=p(y=1|x;\theta)$

在给定参数下，$h$函数判断同意数据为0的概率和为1的概率应该和是1

$P(y=0|x;\theta)+P(y=1|x;\theta)=1$

### 决策界限

根据上图所示，当$g$的参数$\ge0$时，$g$函数的值大于$0.5$，相应的当$g$的参数$\lt0$时，$g$函数的值小于$0.5$,而$g$函数的参数又是$\theta^TX$，所以我们可以得到结论当$\theta^TX\ge0$时，$h_{\theta}(x)\ge0.5$，当$\theta^TX<0$时，$h_{\theta}(x)>0.5$

![决策界限](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/decision boundary.4ro0dxvzpug0.png)

图上两种分类中间的直线就是决策界限

### 代价函数

原先的代价函数可以写成 $J(\theta)=Cost(h_{\theta}(x^{(i)}),y^{(i)})=\frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^2$

因为$h$函数是一个复杂的函数$h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}$，导致代价函数$J$是一个非凹函数，这样在梯度下降的过程中不一定会找到全局最小值

![no-convex](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/no-convex.43lq2vuti0c0.png)

很显然这个代价函数并不适合逻辑回归，这里将介绍一个新的代价函数

$Cost(h_{\theta}(x),y) = \left\{ \begin{array}{**lr**} -\log(h_{\theta}(x)) & if\ y = 1  \\ -\log(1-h_{\theta}(x)) & if\ y = 0 \end{array} \right. $

当$y=1,h_{\theta}(x) =1$时$Cost$为0，如果$h_{\theta}(x)\to0$ ，那么$Cost\to \infty$

当$y=0,h_{\theta}(x)=0$时$Cost$为1，如果$h_{\theta}(x)\to1$，那么$Cost\to \infty$

### 简化代价函数

$J(\theta)=\frac{1}{m}\sum\limits_{i=1}^mCost(h_{\theta}(x^{(i)}), y^{(i)})$

$Cost(h_{\theta}(x),y) = \left\{ \begin{array}{lr} -\log(h_{\theta}(x)) & if\ y = 1  \\ -\log(1-h_{\theta}(x)) & if\ y = 0 \end{array} \right. $

因为代价函数中包括了两种不同的状态，不利于梯度下降函数的运行，所以我们要简化代价函数

$Cost(h_{\theta}(x),y) = -y \log(h_{\theta}(x)) - (1-y)\log(1-h_{\theta}(x))$

这样我们就将两个式子简化成一行，新的代价函数为：

$J(\theta)=\frac{1}{m}\sum\limits_{i=1}^mCost(h_{\theta}(x^{(i)}), y^{(i)}) \\ = -\frac{1}{m}[\sum\limits_{i=1}^my^{(i)}\log(h_{\theta}(x^{(i)}))+(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))] $

我们利用梯度下降去寻找最小的$\min\limits_{\theta}J(\theta)$,去预测一个新的参数$x$为1的概率

### 梯度下降

逻辑回归的的梯度下降公式跟线性回归的几乎一样

$Repeat \{  \\ \theta_j:=\theta_j-\frac{\alpha}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j \\ \}$

向量式写法：

$\theta := \theta - \frac{\alpha}{m}X^T(g(X\theta)-y)$

### 高级优化

Octave中有许多由于梯度下降的算法，但是你并不需要全部掌握，你可以使用Octave提供的库

比如说`fminunc`你只需要提供所计算的代价函数，每个$\theta$的导数，初始$\theta$即可以使用

![Advanced Optimization](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/Advanced Optimization.13yvc5mix3cw.png)

比如对于图上的代价函数$J$，你需要提供一个代价函数`costFunction`，返回[代价值`jVal`，每个参数的导数`gradient`]

设置高级优化函数的参数`options`,提供初始化的参数`initialTheta`，函数经过计算会返回[最佳的参数`optTheta`, 代价函数的值`functionVal`，是否收敛`exitFlag`]

### 多类别分类

有时候需要你区分的类别可能不止2中，可能有3种，4种或更多

比如，病人带着鼻塞来到诊所，你需要区分是没生病，还是着凉还是流感，这里$y=0$代表没生病，$y=1$代表着凉, $y=2$代表流感

对于这种情况，我们可以将3种类型看成2种，进行3次的逻辑回归

![多类别分类](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/cqmPjanSEeawbAp5ByfpEg_299fcfbd527b6b5a7440825628339c54_Screenshot-2016-11-13-10.52.29.ofas81igcps.png)

我们每次只看1类，$h_{\theta}^{(i)}(x) = P(y=1|x;\theta)$为为这种情况为1的概率

当我们接收一个新的$x$时，我们对于每个逻辑回归最判断，得出最有可能的概率$\max\limits_{i}h_{\theta}^{(i)}(x)$

