---
title: 吴恩达机器学习1
mathjax: true
categories: [机器学习, 学习笔记]
tags: [机器学习, 学习笔记]
abbrlink: 79200c28
date: 2021-05-30 12:12:23
img: https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/wallhaven-72jmve.42eeeqocjso0.jpg
---

吴恩达机器学习笔记(一)

<!-- less -->

## 什么是机器学习

>$A\ computer\ program\ is\ said\ to\ learn\ from\  experience\ E\ with\ respect\ to\ some\ class\ of\ tasks\ T\ and\ performance\ measure\ P\ ,$
>
>$\ if\ its\ performance\ at\ tasks\ in\ T,\ as\ measured\ by\ P,\ improves\ with\ experience\ E.$ 

一个计算机程序可以从经验$E$中学习，执行任务$T$，由$P$做性能评估，并且他在$T$任务中的表现(由$P$衡量)随着经验$E$的提高而提高。

以下跳棋为例:

>$E:$  下过很多跳棋的经验
>
>$T：$下跳棋的任务
>
>$P ：$下一局赢下跳棋的可能性

一般的任何机器学习都可以分类两类：监督学习和非监督学习

### 什么是监督学习

>$In\ supervised\ learning,\ we\ are\ given\ a data\ set\ and\ already\ know\ what\ our\ correct\ output\ should\ look\ like,$
>
>$having\ the\ idea\ that\ there\ is\ a\ relationship\ between\ the\ input\ and\ the\ output.$

在监督学习中，我们已知一个数据集，并且已经知道正确的输出应该是什么样的，我们知道输入与输出之间存在着一种关系。

在监督学习中，我们已经知道数据集的含义、分布，根据已知的信息来预测。

>$Supervised\ learning\ problems\ are\ categorized\ into\ "regression"\ and\ "classification"\ problems.\ $
>
>$In\ a\ regression\ problem,\ we\ are\ trying\ to\ predict\ results\ within\ a\ continuous\ output,\ meaning\ that \ $
>
>$we\ are\ trying\ to\ map\ input\ variables\ to\ some\ continuous\ function.\ In\ a\ classification\ problem,\ we\ $
>
>$ are\  instead\ trying\ to\ predict\ results\ in\ a\ discrete\ output.\ In\ other\ words,\ we\ are\ trying\ to\ map\ input\ $
>
>$variables\ into\ discrete\ categories.\ $

监督学习问题分为“回归”问题和“分类”问题。在回归问题中，我们试图预测连续输出中的结果，这意味着我们试图将输入变量映射到某个连续函数。在分类问题中，我们试图预测离散输出中的结果。换句话说，我们试图将输入变量映射到离散的类别中。

#### 例子

回归问题：给一张照片，去预测照片中人的年龄

分类问题：对于患有肿瘤的病人，去预测肿瘤是良性还是恶性

### 什么是无监督学习

>$Unsupervised\ learning\ allows\ us\ to\ approach\ problems\ with\ little\ or\ no\ idea\ what\ our\ results\ should\  $
>
>$look\ like.\ We\ can\ derive\ structure\ from\ data\ where\ we\ don't\ necessarily\ know\ the\ effect\ of\ the\ variables.\ $
>
>$We\ can\ derive\ this\ structure\ by\ clustering\ the\ data\ based\ on\ relationships\ among\ the\ variables\ in\ the\ data.$
>
>$With\ unsupervised\ learning\ there\ is\ no\ feedback\ based\ on\ the\ prediction\ results.$

无监督学习让我们在几乎不知道结果是什么的情况下处理问题。我们可以从数据中得出结构，而我们并不一定知道变量的影响。

我们可以根据数据中变量之间的关系对数据进行聚类，从而得到这种结构。

在无监督学习中，没有基于预测结果的反馈。

#### 例子

收集100万个不同的基因，然后找到一种方法将这些基因分组，这些分组在某种程度上是相似的，或者由不同的变量(如寿命、位置、角色等)相关的。

## 建立符号规则

$m$ 表示训练集数量，对于训练集中的每一项，我们使用$x^{(i)}$表示训练集中的第$i$行输入的变量，使用$y^{(i)}$表示训练集中第$i$行输出的变量 。

一对$(x^{(i)},y^{(i)})$成为一个训练样例，$(i)$只是表示训练集中的索引，而不是次幂。还使用$X$来表示输入值的空间，$Y$表示输出值的空间。

## 监督学习

>$To\ describe\ the\ supervised\ learning\ problem\ slightly\ more\ formally,\ our\ goal\ is,\ given\ a\ training\ set,\ $
>
>$to\ learn\ a\ function\ h\ :\ X\ →\ Y\ so\ that\ h(x)\ is\ a\ “good”\ predictor\ for\ the\ corresponding\ value\ of\ y.\ $
>
>$For\ historical\ reasons,\ this\ function\ h\ is\ called\ a\ hypothesis.\ Seen\ pictorially,\ the\ process\ is\ $
>
> $therefore\ like\ this:\ $

为了更正式地描述监督学习问题，我们的目标是，给定一个训练集，学习一个函数$h: X → Y$，使$h(X)$是对应的$Y$值的一个很好的预测器。由于历史原因，这个函数$h$被称为一个假设。从图片上看，这个过程是这样的

![supervised learning process](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/supervised learning process.3tlcgn2ah0o0.png)

### 代价函数

假设的$h$函数由$h_{\theta}(x)=\theta_0 + \theta_1x$拟合而来，$\theta_0$和$\theta_1$是两个未知参数

我们的目的是选择最适合的$\theta_0,\theta_1$以便于$h_{\theta}(x)$最接近与真正的训练集对应$Y$值的分布

代价函数$J(\theta_0,\theta_1)=\frac{1}{2m}\sum\limits_{i=1}^m(\hat y_i -y_i)^2=\frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$表示$h$函数与$Y$值之间的误差

$Goal:\ \min\limits_{\theta_0,\theta_1}J(\theta_0, \theta_1)$

把$\theta_0$设成0,不同的$\theta_1$对应的$h$函数所形成的误差在$J(\theta_1)$的图像上呈现一个二次函数的图像，寻找$J(\theta_1)$的最小值，就是寻找最适合，最拟合与$Y$的$h(\theta_1)$函数

![Cost Function](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/Cost Function.x637adlj7nk.png)

### 梯度下降

梯度下降算法就是根据当前的参数和代价函数选择一个下降方向，最终达到局部最优解的一种算法。

> $The\ way\ we\ do\ this\ is\ by\ taking\ the\ derivative\ (the\ tangential$
>
> $line\ to\ a\ function)\ of\ our\ cost\ function.\ The\ slope\ of\ the\ tangent$
>
>  $is\ the\ derivative\ at\ that\ point\ and\ it\ will\ give\ us\ a\ direction$
>
> $to\ move\ towards.\ We\ make\ steps\ down\ the\ cost\ function\ in\ the$
>
> $direction\ with\ the\ steepest\ descent.$

梯度下降算法是求当前代价函数的导数（也就是函数图像的切线），这一点的导数会给我们一个方向，我们按下降最陡的方向逐步降低成本函数。

> $The\ size\ of\ each\ step\ is\ determined\ by\ the\ parameter\ α,\ which\ is$
>
> $called\ the\ learning\ rate.$

每个步骤的大小由参数α决定，称为学习率。

$repeat\ until\ convergence\ \{ \\ \theta_j\ :=\  \theta_j\ - \alpha \ \frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) (for j=0 and j=1) \\ \}$

$\theta_0$和$\theta_1$同时更新，而不是先更新$\theta_0$，然后再更新$\theta_1$

![更新方法](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/yr-D1aDMEeai9RKvXdDYag_627e5ab52d5ff941c0fcc741c2b162a0_Screenshot-2016-11-02-00.19.56.6zxmtif8pt40.png)

#### 梯度下降如何达到最优点

我们现在使用一个简单函数来解释一下梯度下降算法

![梯度下降如何达到最优点](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/SMSIxKGUEeav5QpTGIv-Pg_ad3404010579ac16068105cfdc8e950a_Screenshot-2016-11-03-00.05.06.7ce3ttmbrrc.png)

假设我们的参数只有一个

$\theta_1 :=\theta_1-\alpha \frac{\partial}{\partial\theta_1}J(\theta_0)$

图上第一个例子：当所在点在最优解右侧时，斜率为正$\theta$减去的时正值，$\theta$向左边移动

图上第二个例子：当所在点在最优解左侧时，斜率为正$\theta$减去的时负值，$\theta$向右边移动

根据斜率（也就是导数）移动，可以移动到当前可以到达的最优点

#### 步距

![步距](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/UJpiD6GWEeai9RKvXdDYag_3c3ad6625a2a4ec8456f421a2f4daf2e_Screenshot-2016-11-03-00.05.27.416l0lpgujs0.png)

当我们的步距过大和过小时都会出现问题

过小时移动次数太多，移动距离太小

过大时会越过最优点

#### 梯度下降如何与固定步长相结合

![梯度下降如何与固定步长相结合](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/RDcJ-KGXEeaVChLw2Vaaug_cb782d34d272321e88f202940c36afe9_Screenshot-2016-11-03-00.06.00.4t7xs1l77fc0.png)

当点越接近最低点时，导数最小，到最低点时导数为零，这就控制了步长使得步长的移动合适

#### 梯度下降与代价函数结合

拟合函数：$h(\theta_0,\theta_1)=\theta_0+\theta_1x$

代价函数: $J(\theta_0, \theta_1) = \frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$

梯度下降:$repeat\ until\ convergence\ \{ \\ \ \ \ \  \theta_j\ :=\  \theta_j\ - \alpha \ \frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) (for j=0 and j=1) \\ \} $

结合一下: $\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\\ = \frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2 \\ = \frac{1}{2m}\sum\limits_{i=1}^m(\theta_0+\theta_1(x^{(i)})-y^{(i)})^2$

经过简单的微分过程:$\theta_0\  j = 0 \ :\ \ \frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)= \frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)}) \\  \theta_1\  j = 1 \ :\ \ \frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)= \frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}((x^{(i)})-y^{(i)}) \cdot x^{(i)}) $

$m$是训练集的大小，$\theta_0$是常量它会随着$\theta_1$同步变化

这种方法在每一步上看整个训练集中的每个例子，称为批梯度下降。