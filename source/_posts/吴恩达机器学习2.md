---
title: 吴恩达机器学习2
mathjax: true
img: >-
  https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/cover/wallhaven-j3w3lq.5vbu53bm59o0.png
categories:
  - 机器学习
  - 学习笔记
tags:
  - 机器学习
  - 学习笔记
abbrlink: e0295d92
date: 2021-06-03 21:26:40
---

吴恩达机器学习笔记(二)

<!-- less -->

## 多个特征值

对于多个特征的线性回归，我们使用

$m$：代表训练集的大小

$n$：代表特性的数量

$x^{(i)}$：代表第$i$组数据

$x^{(i)}_j$：代表第$i$组数据中的第$j$个特征

对于多个特性的线性回归公式

$h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n$

我们把参数和特征值看成两个向量

$\theta=\begin{bmatrix}\theta_0 \\ \theta_1 \\  \dots \\ \theta_n \end{bmatrix} ,X = \begin{bmatrix}x_0\\ x_1\\ \dots \\x_n \end{bmatrix}(x_0=1),$

那么$h$就可以看成

$h_{\theta}(x) = \theta^{T}X$

## 多个特征值的梯度下降

线性回归：$h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n=\theta^TX$

代价函数：$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$

梯度下降跟原来$n=1$的时形式相同，只不过扩展了一下

梯度下降：$Repeat \{  \\ \theta_j:=\theta_j-\alpha\frac{\alpha}{\alpha\theta_j}J(\theta) \\ \}$ 

经过求偏导以后梯度下降变为:

$Repeat \{  \\ \theta_j:=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)_j} \\ \}$

## 特征放缩

- 尽量保证多个特征的值都处于一个相近的范围
- 更一般的将特征放缩到$-1\le x_i \le 1$
- 如果一个特征的范围再$-3\le x_i \le 3$是可以接收的

### 如何特征放缩

一般的使用：$x_i:=\frac{x_i-\mu_i}{s_i}$来进行特征缩放

$\mu_i$：一般表示训练集中特征$i$的平均值

$s_i$：一般表示训练集中特征$i$的范围(最大值-最小值)

## $\alpha$的调整

画一个代价函数随着迭代次数的增加，$J(\theta)$的值的变化图，确保$J(\theta)$随着迭代次数的增加而减小，如果不是，就需要考虑是否再减小$\alpha$

![代价函数迭代次数](https://cdn.jsdelivr.net/gh/Mug-9/imge-stroage@master/Andrew-ML/FEfS3aajEea3qApInhZCFg_6be025f7ad145eb0974b244a7f5b3f59_Screenshot-2016-11-09-09.35.59.4ikm8ukc2ju0.png)

但是如果$\alpha$太小，那么$J(\theta)$的收敛就会变得非常缓慢

### 收敛

如果$J(\theta)$在一次迭代中减小到小于$E$，那么证明$J(\theta)$已经收敛，$E$是某个小值，如$10^{-3}$。但在实际应用中，这个阈值的选择比较困难。

## 多项式回归

有时线性回归不足以拟合训练集中的数据，这时我们可以选择多项式回归。

我们可以改变我们的假设函数的行为或曲线，使它成为二次，三次或平方根函数(或任何其他形式)。

例如我们的假设方法是$h_{\theta}(x)=\theta_0+\theta_1x_1$,我们可以根据$x_1$创建附加的特征使其成为二次函数$h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_1^2$或者三次函数$h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_1^2+\theta_3x^3_1$或者我们也可以使用开平方$h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2\sqrt {x_1}$

值得注意的是，任何形式的参数都需要注意特征放缩使其在一个能够接受的范围
